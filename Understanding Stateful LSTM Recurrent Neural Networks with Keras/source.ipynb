{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Stateful LSTM Recurrent Neural Networks with Keras\n",
    "\n",
    "A powerful and popular recurrent neural network is the long short-term model network or LSTM.\n",
    "\n",
    "It is widely used because the architecture overcomes the vanishing and exploding gradient problem that plagues all recurrent neural networks, allowing very large and very deep networks to be created.\n",
    "\n",
    "Like other recurrent neural networks, LSTM networks maintain state, and the specifics of how this is implemented in Keras framework can be confusing.\n",
    "\n",
    "Here you will discover exactly how state is maintained in LSTM networks by the Keras deep learning library.\n",
    "\n",
    "By the end you will know:\n",
    "\n",
    "- How to develop a naive LSTM network for a sequence prediction problem?\n",
    "- How to carefully manage state through batches and features with an LSTM network?\n",
    "- Hot to manually manage state in an LSTM network for stateful prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description: Learn the Alphabet\n",
    "\n",
    "We are going to develop and contrast a number of different LSTM recurrent neural network models.\n",
    "\n",
    "The context of these comparisons will be a simple sequence prediction problem of learning the alphabet. That is, given a letter of the alphabet, predict the next letter of the alphabet.\n",
    "\n",
    "This is a simple sequence prediction problem that once understood can be generalized to other sequence prediction problems like time series prediction and sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our dataset, the alphabet. We define the alphabet in uppercase characters for readability.\n",
    "\n",
    "Neural networks model numbers, so we need to map the letters of the alphabet to integer values. We can do this easily by creating a dictionary (map) of the letter index to the character. We can also create a reverse lookup for converting predictions back into characters to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create our input and output pairs on which to train our neural network. We can do this by defining an input sequence length, then reading sequences from the input alphabet sequence.\n",
    "\n",
    "For example we use an input length of 1. Starting at the beginning of the raw input data, we can read off the first letter “A” and the next letter as the prediction “B”. We move along one character and repeat until we reach a prediction of “Z”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the NumPy array into a format expected by the LSTM networks, that is [samples, time steps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once reshaped, we can then normalize the input integers to the range 0-to-1, the range of the sigmoid activation functions used by the LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can think of this problem as a sequence classification task, where each of the 26 letters represents a different class. As such, we can convert the output (y) to a one hot encoding, using the Keras built-in function to_categorical()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit different LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive LSTM for Learning One-Char to One-Char Mapping\n",
    "\n",
    "Let’s start off by designing a simple LSTM to learn how to predict the next character in the alphabet given the context of just one character.\n",
    "\n",
    "We will frame the problem as a random collection of one-letter input to one-letter output pairs. As we will see this is a difficult framing of the problem for the LSTM to learn.\n",
    "\n",
    "Let’s define an LSTM network with 32 units and a single output neuron with a softmax activation function for making predictions. Because this is a multi-class classification problem, we can use the log loss function (called **“categorical_crossentropy”** in Keras), and optimize the network using the ADAM optimization function.\n",
    "\n",
    "The model is fit over 500 epochs with a batch size of 1.\n",
    "\n",
    "<pre>\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=500, batch_size=1, verbose=0)\n",
    "</pre>\n",
    "\n",
    "After we fit the model we can evaluate and summarize the performance on the entire training dataset.\n",
    "\n",
    "<pre>\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "</pre>\n",
    "\n",
    "We can then re-run the training data through the network and generate predictions, converting both the input and output pairs back into their original character format to get a visual idea of how well the network learned the problem.\n",
    "\n",
    "<pre>\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result\n",
    "</pre>\n",
    "\n",
    "The entire code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "Model Accuracy: 84.00%\n",
      "['A'] -> B\n",
      "['B'] -> B\n",
      "['C'] -> D\n",
      "['D'] -> E\n",
      "['E'] -> F\n",
      "['F'] -> G\n",
      "['G'] -> H\n",
      "['H'] -> I\n",
      "['I'] -> J\n",
      "['J'] -> K\n",
      "['K'] -> L\n",
      "['L'] -> M\n",
      "['M'] -> N\n",
      "['N'] -> O\n",
      "['O'] -> P\n",
      "['P'] -> Q\n",
      "['Q'] -> R\n",
      "['R'] -> S\n",
      "['S'] -> T\n",
      "['T'] -> U\n",
      "['U'] -> V\n",
      "['V'] -> X\n",
      "['W'] -> Z\n",
      "['X'] -> Z\n",
      "['Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# Naive LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "    \n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=500, batch_size=1, verbose=0)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this problem is indeed difficult for the network to learn.\n",
    "\n",
    "The reason is, the poor LSTM units do not have any context to work with. Each input-output pattern is shown to the network in a random order and the state of the network is reset after each pattern (each batch where each batch contains one pattern).\n",
    "\n",
    "This is abuse of the LSTM network architecture, treating it like a standard multilayer Perceptron.\n",
    "\n",
    "Next, let’s try a different framing of the problem in order to provide more sequence to the network from which to learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive LSTM for a Three-Char Feature Window to One-Char Mapping\n",
    "\n",
    "A popular approach to adding more context to data for multilayer Perceptrons is to use the window method.\n",
    "\n",
    "This is where previous steps in the sequence are provided as additional input features to the network. We can try the same trick to provide more context to the LSTM network.\n",
    "\n",
    "Here, we increase the sequence length from 1 to 3, for example:\n",
    "\n",
    "<pre>\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "</pre>\n",
    "\n",
    "Which creates training patterns like:\n",
    "\n",
    "<pre>\n",
    "ABC -> D\n",
    "BCD -> E\n",
    "CDE -> F\n",
    "</pre>\n",
    "\n",
    "Each element in the sequence is then provided as a new input feature to the network. This requires a modification of how the input sequences reshaped in the data preparation step:\n",
    "\n",
    "<pre>\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "</pre>\n",
    "\n",
    "It also requires a modification for how the sample patterns are reshaped when demonstrating predictions from the model.\n",
    "\n",
    "<pre>\n",
    "x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "</pre>\n",
    "\n",
    "The entire code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n",
      "Model Accuracy: 86.96%\n",
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> V\n",
      "['T', 'U', 'V'] -> X\n",
      "['U', 'V', 'W'] -> Z\n",
      "['V', 'W', 'X'] -> Z\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# Naive LSTM to learn three-char window to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "    \n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=500, batch_size=1, verbose=0)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a small lift in performance that may or may not be real. This is a simple problem that we were still not able to learn with LSTMs even with the window method.\n",
    "\n",
    "Again, this is a misuse of the LSTM network by a poor framing of the problem. Indeed, the sequences of letters are time steps of one feature rather than one time step of separate features. We have given more context to the network, but not more sequence as it expected.\n",
    "\n",
    "In the next section, we will give more context to the network in the form of time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive LSTM for a Three-Char Time Step Window to One-Char Mapping\n",
    "\n",
    "In Keras, the intended use of LSTMs is to provide context in the form of time steps, rather than windowed features like with other network types.\n",
    "\n",
    "We can take our first example and simply change the sequence length from 1 to 3.\n",
    "\n",
    "<pre>\n",
    "seq_length = 3\n",
    "</pre>\n",
    "\n",
    "Again, this creates input-output pairs that look like:\n",
    "\n",
    "<pre>\n",
    "ABC -> D\n",
    "BCD -> E\n",
    "CDE -> F\n",
    "DEF -> G\n",
    "</pre>\n",
    "\n",
    "The difference is that the reshaping of the input data takes the sequence as a time step sequence of one feature, rather than a single time step of multiple features.\n",
    "\n",
    "<pre>\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "</pre>\n",
    "\n",
    "This is the correct intended use of providing sequence context to your LSTM in Keras. The full code example is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n",
      "Model Accuracy: 100.00%\n",
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> V\n",
      "['T', 'U', 'V'] -> W\n",
      "['U', 'V', 'W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# Naive LSTM to learn three-char time steps to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "    \n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=500, batch_size=1, verbose=0)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model learns the problem perfectly as evidenced by the model evaluation and the example predictions.\n",
    "\n",
    "But it has learned a simpler problem. Specifically, it has learned to predict the next letter from a sequence of three letters in the alphabet. It can be shown any random sequence of three letters from the alphabet and predict the next letter.\n",
    "\n",
    "It can not actually enumerate the alphabet. I expect that a larger enough multilayer perception network might be able to learn the same mapping using the window method.\n",
    "\n",
    "The LSTM networks are stateful. They should be able to learn the whole alphabet sequence, but by default the Keras implementation resets the network state after each training batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM State Within A Batch\n",
    "\n",
    "The Keras implementation of LSTMs resets the state of the network after each batch.\n",
    "\n",
    "This suggests that if we had a batch size large enough to hold all input patterns and if all the input patterns were ordered sequentially, that the LSTM could use the context of the sequence within the batch to better learn the sequence.\n",
    "\n",
    "We can demonstrate this easily by modifying the first example for learning a one-to-one mapping and increasing the batch size from 1 to the size of the training dataset.\n",
    "\n",
    "Additionally, Keras shuffles the training dataset before each training epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.\n",
    "\n",
    "<pre>\n",
    "model.fit(X, y, nb_epoch=5000, batch_size=len(dataX), verbose=2, shuffle=False)\n",
    "</pre>\n",
    "\n",
    "The network will learn the mapping of characters using the the within-batch sequence, but this context will not be available to the network when making predictions. We can evaluate both the ability of the network to make predictions randomly and in sequence.\n",
    "\n",
    "The full code example is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "Model Accuracy: 100.00%\n",
      "['A'] -> B\n",
      "['B'] -> C\n",
      "['C'] -> D\n",
      "['D'] -> E\n",
      "['E'] -> F\n",
      "['F'] -> G\n",
      "['G'] -> H\n",
      "['H'] -> I\n",
      "['I'] -> J\n",
      "['J'] -> K\n",
      "['K'] -> L\n",
      "['L'] -> M\n",
      "['M'] -> N\n",
      "['N'] -> O\n",
      "['O'] -> P\n",
      "['P'] -> Q\n",
      "['Q'] -> R\n",
      "['R'] -> S\n",
      "['S'] -> T\n",
      "['T'] -> U\n",
      "['U'] -> V\n",
      "['V'] -> W\n",
      "['W'] -> X\n",
      "['X'] -> Y\n",
      "['Y'] -> Z\n",
      "Test a Random Pattern:\n",
      "['J'] -> K\n",
      "['E'] -> F\n",
      "['T'] -> U\n",
      "['P'] -> Q\n",
      "['B'] -> C\n",
      "['N'] -> O\n",
      "['U'] -> V\n",
      "['O'] -> P\n",
      "['Q'] -> R\n",
      "['T'] -> U\n",
      "['Y'] -> Z\n",
      "['M'] -> N\n",
      "['M'] -> N\n",
      "['P'] -> Q\n",
      "['R'] -> S\n",
      "['S'] -> T\n",
      "['A'] -> B\n",
      "['V'] -> W\n",
      "['H'] -> I\n",
      "['V'] -> W\n"
     ]
    }
   ],
   "source": [
    "# Naive LSTM to learn one-char to one-char mapping with all data in each batch\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "    \n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=seq_length, dtype='float32')\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (X.shape[0], seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=5000, batch_size=len(dataX), verbose=0, shuffle=False)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result\n",
    "    \n",
    "# demonstrate predicting random patterns\n",
    "print \"Test a Random Pattern:\"\n",
    "for i in range(0,20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the network is able to use the within-sequence context to learn the alphabet, achieving 100% accuracy on the training data.\n",
    "\n",
    "Importantly, the network can make accurate predictions for the next letter in the alphabet for randomly selected characters. Very impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful LSTM for a One-Char to One-Char Mapping\n",
    "\n",
    "We have seen that we can break-up our raw data into fixed size sequences and that this representation can be learned by the LSTM, but only to learn random mappings of 3 characters to 1 character.\n",
    "\n",
    "We have also seen that we can pervert batch size to offer more sequence to the network, but only during training.\n",
    "\n",
    "Ideally, we want to expose the network to the entire sequence and let it learn the inter-dependencies, rather than us define those dependencies explicitly in the framing of the problem.\n",
    "\n",
    "We can do this in Keras by making the LSTM layers stateful and manually resetting the state of the network at the end of the epoch, which is also the end of the training sequence.\n",
    "\n",
    "This is truly how the LSTM networks are intended to be used. We find that by allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half).\n",
    "\n",
    "We first need to define our LSTM layer as stateful. In so doing, we must explicitly specify the batch size as a dimension on the input shape. This also means that when we evaluate the network or make predictions, we must also specify and adhere to this same batch size. This is not a problem now as we are using a batch size of 1. This could introduce difficulties when making predictions when the batch size is not one as predictions will need to be made in batch and in sequence.\n",
    "\n",
    "<pre>\n",
    "batch_size = 1\n",
    "model.add(LSTM(16, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "</pre>\n",
    "\n",
    "An important difference in training the stateful LSTM is that we train it manually one epoch at a time and reset the state after each epoch. We can do this in a for loop. Again, we do not shuffle the input, preserving the sequence in which the input training data was created.\n",
    "\n",
    "<pre>\n",
    "for i in range(300):\n",
    "    model.fit(X, y, nb_epoch=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()\n",
    "</pre>\n",
    "\n",
    "As mentioned, we specify the batch size when evaluating the performance of the network on the entire training dataset.\n",
    "\n",
    "<pre>\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "</pre>\n",
    "\n",
    "Finally, we can demonstrate that the network has indeed learned the entire alphabet. We can seed it with the first letter “A”, request a prediction, feed the prediction back in as an input, and repeat the process all the way to “Z”.\n",
    "\n",
    "<pre>\n",
    "# demonstrate some model predictions\n",
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "    x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "    seed = [index]\n",
    "model.reset_states()\n",
    "</pre>\n",
    "\n",
    "We can also see if the network can make predictions starting from an arbitrary letter.\n",
    "\n",
    "<pre>\n",
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print \"New start: \", letter\n",
    "for i in range(0, 5):\n",
    "    x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "    seed = [index]\n",
    "model.reset_states()\n",
    "</pre>\n",
    "\n",
    "The entire code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "Model Accuracy: 96.00%\n",
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> H\n",
      "H -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "New start:  K\n",
      "K -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n"
     ]
    }
   ],
   "source": [
    "# Stateful LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "    \n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "for i in range(300):\n",
    "    model.fit(X, y, nb_epoch=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    model.reset_states()\n",
    "    \n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "    x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "    seed = [index]\n",
    "model.reset_states()\n",
    "\n",
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print \"New start: \", letter\n",
    "for i in range(0, 5):\n",
    "    x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "    seed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the network has memorized the entire alphabet perfectly. It used the context of the samples themselves and learned whatever dependency it needed to predict the next character in the sequence.\n",
    "\n",
    "We can also see that if we seed the network with the first letter, that it can correctly rattle off the rest of the alphabet.\n",
    "\n",
    "We can also see that it has only learned the full alphabet sequence and that from a cold start. When asked to predict the next letter from “K” that it predicts “B” and falls back into regurgitating the entire alphabet.\n",
    "\n",
    "To truly predict “K” the state of the network would need to be warmed up iteratively fed the letters from “A” to “J”. This tells us that we could achieve the same effect with a “stateless” LSTM by preparing training data like:\n",
    "\n",
    "<pre>\n",
    "---a -> b\n",
    "--ab -> c\n",
    "-abc -> d\n",
    "abcd -> e\n",
    "</pre>\n",
    "\n",
    "Where the input sequence is fixed at 25 (a-to-y to predict z) and patterns are prefixed with zero-padding.\n",
    "\n",
    "Finally, this raises the question of training an LSTM network using variable length input sequences to predict the next character.\n",
    "\n",
    "## LSTM with Variable-Length Input to One-Char Output\n",
    "\n",
    "In the previous section, we discovered that the Keras “stateful” LSTM was really only a shortcut to replaying the first n-sequences, but didn’t really help us learn a generic model of the alphabet.\n",
    "\n",
    "In this section we explore a variation of the “stateless” LSTM that learns random subsequences of the alphabet and an effort to build a model that can be given arbitrary letters or subsequences of letters and predict the next letter in the alphabet.\n",
    "\n",
    "Firstly, we are changing the framing of the problem. To simplify we will define a maximum input sequence length and set it to a small value like 5 to speed up training. This defines the maximum length of subsequences of the alphabet will be drawn for training. In extensions, this could just as set to the full alphabet (26) or longer if we allow looping back to the start of the sequence.\n",
    "\n",
    "We also need to define the number of random sequences to create, in this case 1000. This too could be more or less. I expect less patterns are actually required.\n",
    "\n",
    "<pre>\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print sequence_in, '->', sequence_out\n",
    "</pre>\n",
    "\n",
    "Running this code in the broader context will create input patterns that look like the following:\n",
    "\n",
    "<pre>\n",
    "PQRST -> U\n",
    "W -> X\n",
    "O -> P\n",
    "OPQ -> R\n",
    "IJKLM -> N\n",
    "QRSTU -> V\n",
    "ABCD -> E\n",
    "X -> Y\n",
    "GHIJ -> K\n",
    "</pre>\n",
    "\n",
    "The input sequences vary in length between 1 and max_len and therefore require zero padding. Here, we use left-hand-side (prefix) padding with the Keras built in pad_sequences() function.\n",
    "\n",
    "<pre>\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "</pre>\n",
    "\n",
    "The trained model is evaluated on randomly selected input patterns. This could just as easily be new randomly generated sequences of characters. I also believe this could also be a linear sequence seeded with “A” with outputs fes back in as single character inputs.\n",
    "\n",
    "The full code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQRST -> U\n",
      "W -> X\n",
      "O -> P\n",
      "OPQ -> R\n",
      "IJKLM -> N\n",
      "QRSTU -> V\n",
      "ABCD -> E\n",
      "X -> Y\n",
      "GHIJ -> K\n",
      "M -> N\n",
      "XY -> Z\n",
      "QRST -> U\n",
      "ABC -> D\n",
      "JKLMN -> O\n",
      "OP -> Q\n",
      "XY -> Z\n",
      "D -> E\n",
      "T -> U\n",
      "B -> C\n",
      "QRSTU -> V\n",
      "HIJ -> K\n",
      "JKLM -> N\n",
      "ABCDE -> F\n",
      "X -> Y\n",
      "V -> W\n",
      "DE -> F\n",
      "DEFG -> H\n",
      "BCDE -> F\n",
      "EFGH -> I\n",
      "BCDE -> F\n",
      "FG -> H\n",
      "RST -> U\n",
      "TUV -> W\n",
      "STUV -> W\n",
      "LMN -> O\n",
      "P -> Q\n",
      "MNOP -> Q\n",
      "JK -> L\n",
      "MNOP -> Q\n",
      "OPQRS -> T\n",
      "UVWXY -> Z\n",
      "PQRS -> T\n",
      "D -> E\n",
      "EFGH -> I\n",
      "IJK -> L\n",
      "WX -> Y\n",
      "STUV -> W\n",
      "MNOPQ -> R\n",
      "P -> Q\n",
      "WXY -> Z\n",
      "VWX -> Y\n",
      "V -> W\n",
      "HI -> J\n",
      "KLMNO -> P\n",
      "UV -> W\n",
      "JKL -> M\n",
      "ABCDE -> F\n",
      "WXY -> Z\n",
      "M -> N\n",
      "CDEF -> G\n",
      "KLMNO -> P\n",
      "RST -> U\n",
      "RS -> T\n",
      "W -> X\n",
      "J -> K\n",
      "WX -> Y\n",
      "JKLMN -> O\n",
      "MN -> O\n",
      "L -> M\n",
      "BCDE -> F\n",
      "TU -> V\n",
      "MNOPQ -> R\n",
      "NOPQR -> S\n",
      "HIJ -> K\n",
      "JKLM -> N\n",
      "STUVW -> X\n",
      "QRST -> U\n",
      "N -> O\n",
      "VWXY -> Z\n",
      "B -> C\n",
      "UVWX -> Y\n",
      "OP -> Q\n",
      "K -> L\n",
      "C -> D\n",
      "X -> Y\n",
      "ST -> U\n",
      "JKLM -> N\n",
      "B -> C\n",
      "QR -> S\n",
      "RS -> T\n",
      "VWXY -> Z\n",
      "S -> T\n",
      "NOP -> Q\n",
      "KLMNO -> P\n",
      "IJ -> K\n",
      "EF -> G\n",
      "MNOP -> Q\n",
      "WXY -> Z\n",
      "HI -> J\n",
      "P -> Q\n",
      "STUVW -> X\n",
      "Q -> R\n",
      "MN -> O\n",
      "O -> P\n",
      "C -> D\n",
      "L -> M\n",
      "JKLM -> N\n",
      "K -> L\n",
      "IJKLM -> N\n",
      "FGHIJ -> K\n",
      "LM -> N\n",
      "OPQ -> R\n",
      "U -> V\n",
      "HIJKL -> M\n",
      "PQR -> S\n",
      "S -> T\n",
      "OPQR -> S\n",
      "J -> K\n",
      "DE -> F\n",
      "K -> L\n",
      "BCDE -> F\n",
      "EFGH -> I\n",
      "RSTUV -> W\n",
      "LMNOP -> Q\n",
      "QR -> S\n",
      "ABCDE -> F\n",
      "LM -> N\n",
      "IJKLM -> N\n",
      "B -> C\n",
      "VWX -> Y\n",
      "MNOPQ -> R\n",
      "MNOPQ -> R\n",
      "LM -> N\n",
      "S -> T\n",
      "VWX -> Y\n",
      "WXY -> Z\n",
      "F -> G\n",
      "KLMNO -> P\n",
      "OPQ -> R\n",
      "M -> N\n",
      "X -> Y\n",
      "OPQRS -> T\n",
      "F -> G\n",
      "JKLMN -> O\n",
      "XY -> Z\n",
      "OPQ -> R\n",
      "FG -> H\n",
      "OP -> Q\n",
      "DEFGH -> I\n",
      "ABCD -> E\n",
      "VWX -> Y\n",
      "U -> V\n",
      "UV -> W\n",
      "VWX -> Y\n",
      "LMNO -> P\n",
      "E -> F\n",
      "NOPQ -> R\n",
      "HIJK -> L\n",
      "HIJ -> K\n",
      "DE -> F\n",
      "B -> C\n",
      "UVW -> X\n",
      "STUV -> W\n",
      "RST -> U\n",
      "H -> I\n",
      "I -> J\n",
      "MN -> O\n",
      "CDEF -> G\n",
      "ABC -> D\n",
      "RSTU -> V\n",
      "B -> C\n",
      "JKLM -> N\n",
      "TUVW -> X\n",
      "STUVW -> X\n",
      "C -> D\n",
      "UV -> W\n",
      "QRS -> T\n",
      "ABC -> D\n",
      "NOP -> Q\n",
      "W -> X\n",
      "DE -> F\n",
      "VWXY -> Z\n",
      "UV -> W\n",
      "JK -> L\n",
      "E -> F\n",
      "MNO -> P\n",
      "EFGH -> I\n",
      "PQRS -> T\n",
      "FGH -> I\n",
      "WXY -> Z\n",
      "OPQRS -> T\n",
      "TUV -> W\n",
      "MN -> O\n",
      "O -> P\n",
      "LMN -> O\n",
      "VWX -> Y\n",
      "QR -> S\n",
      "TUV -> W\n",
      "STU -> V\n",
      "EFGH -> I\n",
      "E -> F\n",
      "HIJ -> K\n",
      "QRS -> T\n",
      "H -> I\n",
      "K -> L\n",
      "E -> F\n",
      "UV -> W\n",
      "X -> Y\n",
      "QR -> S\n",
      "QRS -> T\n",
      "WXY -> Z\n",
      "S -> T\n",
      "CDEFG -> H\n",
      "PQRST -> U\n",
      "RST -> U\n",
      "A -> B\n",
      "CDEF -> G\n",
      "X -> Y\n",
      "JKLM -> N\n",
      "VWX -> Y\n",
      "N -> O\n",
      "W -> X\n",
      "TUVW -> X\n",
      "LMNOP -> Q\n",
      "EFG -> H\n",
      "HI -> J\n",
      "WXY -> Z\n",
      "IJK -> L\n",
      "R -> S\n",
      "H -> I\n",
      "V -> W\n",
      "OPQR -> S\n",
      "QRSTU -> V\n",
      "MNOPQ -> R\n",
      "Q -> R\n",
      "VWXY -> Z\n",
      "ABCDE -> F\n",
      "HIJK -> L\n",
      "FGHIJ -> K\n",
      "BC -> D\n",
      "UV -> W\n",
      "WXY -> Z\n",
      "VWX -> Y\n",
      "L -> M\n",
      "FG -> H\n",
      "E -> F\n",
      "WXY -> Z\n",
      "KLMN -> O\n",
      "B -> C\n",
      "QRSTU -> V\n",
      "X -> Y\n",
      "ST -> U\n",
      "GH -> I\n",
      "CDE -> F\n",
      "IJKLM -> N\n",
      "JKL -> M\n",
      "HIJ -> K\n",
      "UVWXY -> Z\n",
      "PQ -> R\n",
      "AB -> C\n",
      "HIJ -> K\n",
      "EFG -> H\n",
      "PQRS -> T\n",
      "BCDEF -> G\n",
      "IJKL -> M\n",
      "DEFGH -> I\n",
      "VW -> X\n",
      "XY -> Z\n",
      "OPQ -> R\n",
      "MN -> O\n",
      "OP -> Q\n",
      "WXY -> Z\n",
      "STU -> V\n",
      "LM -> N\n",
      "UV -> W\n",
      "EF -> G\n",
      "LMN -> O\n",
      "D -> E\n",
      "H -> I\n",
      "KLMNO -> P\n",
      "PQRST -> U\n",
      "V -> W\n",
      "M -> N\n",
      "UVW -> X\n",
      "ABCD -> E\n",
      "LM -> N\n",
      "A -> B\n",
      "DEFGH -> I\n",
      "IJK -> L\n",
      "OP -> Q\n",
      "WXY -> Z\n",
      "CDEFG -> H\n",
      "UVW -> X\n",
      "RS -> T\n",
      "FGHIJ -> K\n",
      "RST -> U\n",
      "NO -> P\n",
      "X -> Y\n",
      "RST -> U\n",
      "I -> J\n",
      "TUV -> W\n",
      "B -> C\n",
      "UVWX -> Y\n",
      "HIJKL -> M\n",
      "MNOPQ -> R\n",
      "ABC -> D\n",
      "PQ -> R\n",
      "WX -> Y\n",
      "XY -> Z\n",
      "UVW -> X\n",
      "IJKL -> M\n",
      "XY -> Z\n",
      "DEFG -> H\n",
      "H -> I\n",
      "Q -> R\n",
      "CDEFG -> H\n",
      "C -> D\n",
      "ABCD -> E\n",
      "LMN -> O\n",
      "PQRST -> U\n",
      "VWX -> Y\n",
      "M -> N\n",
      "KLMN -> O\n",
      "AB -> C\n",
      "NOPQ -> R\n",
      "F -> G\n",
      "NO -> P\n",
      "KLM -> N\n",
      "TUVWX -> Y\n",
      "U -> V\n",
      "CDEFG -> H\n",
      "FGHI -> J\n",
      "STUVW -> X\n",
      "JKLM -> N\n",
      "ABC -> D\n",
      "JKLMN -> O\n",
      "TUVWX -> Y\n",
      "D -> E\n",
      "EFGH -> I\n",
      "IJ -> K\n",
      "UVW -> X\n",
      "OPQR -> S\n",
      "N -> O\n",
      "VWXY -> Z\n",
      "ABC -> D\n",
      "J -> K\n",
      "RS -> T\n",
      "LMNOP -> Q\n",
      "BC -> D\n",
      "OPQ -> R\n",
      "JKLM -> N\n",
      "WX -> Y\n",
      "BCD -> E\n",
      "RSTU -> V\n",
      "GHI -> J\n",
      "O -> P\n",
      "R -> S\n",
      "QR -> S\n",
      "HIJKL -> M\n",
      "UVWXY -> Z\n",
      "CDEFG -> H\n",
      "OP -> Q\n",
      "HIJK -> L\n",
      "A -> B\n",
      "RST -> U\n",
      "QR -> S\n",
      "ABCD -> E\n",
      "LMN -> O\n",
      "TUV -> W\n",
      "MNO -> P\n",
      "AB -> C\n",
      "M -> N\n",
      "OPQR -> S\n",
      "STU -> V\n",
      "TUV -> W\n",
      "PQRST -> U\n",
      "LM -> N\n",
      "A -> B\n",
      "A -> B\n",
      "OPQ -> R\n",
      "HIJK -> L\n",
      "TU -> V\n",
      "QRS -> T\n",
      "WX -> Y\n",
      "BCD -> E\n",
      "ST -> U\n",
      "X -> Y\n",
      "EFGHI -> J\n",
      "E -> F\n",
      "FGHIJ -> K\n",
      "HI -> J\n",
      "ABC -> D\n",
      "NOPQ -> R\n",
      "HIJK -> L\n",
      "B -> C\n",
      "U -> V\n",
      "GH -> I\n",
      "TUVWX -> Y\n",
      "S -> T\n",
      "BCDEF -> G\n",
      "KLM -> N\n",
      "Q -> R\n",
      "CD -> E\n",
      "PQ -> R\n",
      "GH -> I\n",
      "U -> V\n",
      "RST -> U\n",
      "JKLM -> N\n",
      "FGH -> I\n",
      "IJ -> K\n",
      "O -> P\n",
      "X -> Y\n",
      "H -> I\n",
      "DEF -> G\n",
      "QRSTU -> V\n",
      "ABCD -> E\n",
      "IJK -> L\n",
      "GHI -> J\n",
      "QR -> S\n",
      "NOPQR -> S\n",
      "EF -> G\n",
      "PQRST -> U\n",
      "RST -> U\n",
      "X -> Y\n",
      "QR -> S\n",
      "HIJ -> K\n",
      "D -> E\n",
      "AB -> C\n",
      "N -> O\n",
      "QR -> S\n",
      "BCDEF -> G\n",
      "QRS -> T\n",
      "DEF -> G\n",
      "TUV -> W\n",
      "A -> B\n",
      "GHIJ -> K\n",
      "W -> X\n",
      "VWXY -> Z\n",
      "LM -> N\n",
      "OPQ -> R\n",
      "XY -> Z\n",
      "KLM -> N\n",
      "RST -> U\n",
      "OP -> Q\n",
      "VWX -> Y\n",
      "OPQ -> R\n",
      "N -> O\n",
      "M -> N\n",
      "JKL -> M\n",
      "OP -> Q\n",
      "DEF -> G\n",
      "BCD -> E\n",
      "K -> L\n",
      "MN -> O\n",
      "IJKL -> M\n",
      "QR -> S\n",
      "IJKLM -> N\n",
      "U -> V\n",
      "FGH -> I\n",
      "MNOPQ -> R\n",
      "TUVW -> X\n",
      "MN -> O\n",
      "RSTUV -> W\n",
      "VWX -> Y\n",
      "Q -> R\n",
      "DEFGH -> I\n",
      "NO -> P\n",
      "T -> U\n",
      "V -> W\n",
      "ST -> U\n",
      "DEFG -> H\n",
      "RS -> T\n",
      "NOPQ -> R\n",
      "GHIJK -> L\n",
      "QRSTU -> V\n",
      "LMNO -> P\n",
      "IJK -> L\n",
      "PQRST -> U\n",
      "IJK -> L\n",
      "DE -> F\n",
      "CD -> E\n",
      "JKLM -> N\n",
      "WX -> Y\n",
      "UV -> W\n",
      "W -> X\n",
      "KLM -> N\n",
      "PQ -> R\n",
      "W -> X\n",
      "WXY -> Z\n",
      "EFGHI -> J\n",
      "E -> F\n",
      "NOP -> Q\n",
      "VW -> X\n",
      "EFGHI -> J\n",
      "NO -> P\n",
      "HIJKL -> M\n",
      "UVWXY -> Z\n",
      "OPQ -> R\n",
      "P -> Q\n",
      "H -> I\n",
      "O -> P\n",
      "GHIJK -> L\n",
      "S -> T\n",
      "E -> F\n",
      "KLMN -> O\n",
      "TUVW -> X\n",
      "E -> F\n",
      "CDE -> F\n",
      "I -> J\n",
      "CDEF -> G\n",
      "F -> G\n",
      "ABCD -> E\n",
      "H -> I\n",
      "LMNOP -> Q\n",
      "V -> W\n",
      "W -> X\n",
      "BCD -> E\n",
      "TU -> V\n",
      "VWXY -> Z\n",
      "UVWX -> Y\n",
      "JKL -> M\n",
      "VW -> X\n",
      "CDEF -> G\n",
      "DEF -> G\n",
      "ABCDE -> F\n",
      "MNO -> P\n",
      "EFGH -> I\n",
      "JKLM -> N\n",
      "QR -> S\n",
      "ABCDE -> F\n",
      "OPQR -> S\n",
      "DEF -> G\n",
      "Q -> R\n",
      "TU -> V\n",
      "CDEFG -> H\n",
      "KLMN -> O\n",
      "VW -> X\n",
      "HIJKL -> M\n",
      "DE -> F\n",
      "OP -> Q\n",
      "I -> J\n",
      "GHIJK -> L\n",
      "HIJKL -> M\n",
      "I -> J\n",
      "AB -> C\n",
      "DE -> F\n",
      "I -> J\n",
      "O -> P\n",
      "HIJK -> L\n",
      "QR -> S\n",
      "MN -> O\n",
      "I -> J\n",
      "LM -> N\n",
      "VWXY -> Z\n",
      "JKLMN -> O\n",
      "BC -> D\n",
      "MN -> O\n",
      "GHIJ -> K\n",
      "KL -> M\n",
      "TU -> V\n",
      "QRST -> U\n",
      "ABCDE -> F\n",
      "GH -> I\n",
      "Q -> R\n",
      "NO -> P\n",
      "RST -> U\n",
      "BCDE -> F\n",
      "T -> U\n",
      "TUV -> W\n",
      "FGHIJ -> K\n",
      "T -> U\n",
      "BCD -> E\n",
      "NO -> P\n",
      "JK -> L\n",
      "BCD -> E\n",
      "G -> H\n",
      "A -> B\n",
      "GHIJK -> L\n",
      "QRSTU -> V\n",
      "AB -> C\n",
      "VW -> X\n",
      "HIJKL -> M\n",
      "FGHIJ -> K\n",
      "PQ -> R\n",
      "UV -> W\n",
      "F -> G\n",
      "A -> B\n",
      "Q -> R\n",
      "MNOP -> Q\n",
      "UVWXY -> Z\n",
      "GHIJK -> L\n",
      "GHIJK -> L\n",
      "BCDE -> F\n",
      "QRS -> T\n",
      "PQRS -> T\n",
      "PQ -> R\n",
      "HI -> J\n",
      "PQRST -> U\n",
      "OPQR -> S\n",
      "QRST -> U\n",
      "IJKLM -> N\n",
      "Q -> R\n",
      "F -> G\n",
      "QRST -> U\n",
      "ST -> U\n",
      "MN -> O\n",
      "CD -> E\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "R -> S\n",
      "C -> D\n",
      "RSTUV -> W\n",
      "KL -> M\n",
      "HIJK -> L\n",
      "CD -> E\n",
      "FGHI -> J\n",
      "VW -> X\n",
      "P -> Q\n",
      "C -> D\n",
      "DE -> F\n",
      "DE -> F\n",
      "I -> J\n",
      "LMNOP -> Q\n",
      "KLMNO -> P\n",
      "QRS -> T\n",
      "F -> G\n",
      "UVWXY -> Z\n",
      "QRS -> T\n",
      "BCD -> E\n",
      "FG -> H\n",
      "ABCDE -> F\n",
      "U -> V\n",
      "M -> N\n",
      "KLMN -> O\n",
      "RST -> U\n",
      "UVWX -> Y\n",
      "X -> Y\n",
      "XY -> Z\n",
      "I -> J\n",
      "KLMN -> O\n",
      "X -> Y\n",
      "W -> X\n",
      "RSTUV -> W\n",
      "VW -> X\n",
      "XY -> Z\n",
      "T -> U\n",
      "CDE -> F\n",
      "FGHI -> J\n",
      "PQ -> R\n",
      "OPQRS -> T\n",
      "D -> E\n",
      "E -> F\n",
      "EFGH -> I\n",
      "GHIJK -> L\n",
      "L -> M\n",
      "KLMN -> O\n",
      "STU -> V\n",
      "EF -> G\n",
      "UV -> W\n",
      "K -> L\n",
      "QRS -> T\n",
      "QRSTU -> V\n",
      "DEF -> G\n",
      "UV -> W\n",
      "D -> E\n",
      "BC -> D\n",
      "OPQRS -> T\n",
      "EFGH -> I\n",
      "QRST -> U\n",
      "EF -> G\n",
      "RST -> U\n",
      "JKL -> M\n",
      "STU -> V\n",
      "UVWX -> Y\n",
      "EFGHI -> J\n",
      "JKLMN -> O\n",
      "P -> Q\n",
      "BCD -> E\n",
      "TU -> V\n",
      "O -> P\n",
      "RST -> U\n",
      "D -> E\n",
      "VWXY -> Z\n",
      "R -> S\n",
      "P -> Q\n",
      "CDE -> F\n",
      "X -> Y\n",
      "UVWXY -> Z\n",
      "DEFGH -> I\n",
      "NOP -> Q\n",
      "ABCD -> E\n",
      "B -> C\n",
      "BC -> D\n",
      "VW -> X\n",
      "E -> F\n",
      "TUVW -> X\n",
      "JKL -> M\n",
      "XY -> Z\n",
      "LM -> N\n",
      "PQRS -> T\n",
      "O -> P\n",
      "KLMN -> O\n",
      "STUV -> W\n",
      "K -> L\n",
      "UVWX -> Y\n",
      "U -> V\n",
      "HIJ -> K\n",
      "W -> X\n",
      "VWXY -> Z\n",
      "WX -> Y\n",
      "HIJ -> K\n",
      "O -> P\n",
      "QR -> S\n",
      "VWXY -> Z\n",
      "CD -> E\n",
      "KL -> M\n",
      "DEFGH -> I\n",
      "LMN -> O\n",
      "QRS -> T\n",
      "JKLMN -> O\n",
      "QR -> S\n",
      "CD -> E\n",
      "QRST -> U\n",
      "BCDEF -> G\n",
      "CDE -> F\n",
      "LMN -> O\n",
      "DEF -> G\n",
      "BCD -> E\n",
      "UV -> W\n",
      "STUVW -> X\n",
      "RS -> T\n",
      "ABCD -> E\n",
      "BCDEF -> G\n",
      "Q -> R\n",
      "UVWXY -> Z\n",
      "VW -> X\n",
      "VW -> X\n",
      "WXY -> Z\n",
      "NOPQR -> S\n",
      "V -> W\n",
      "LM -> N\n",
      "B -> C\n",
      "JKL -> M\n",
      "DE -> F\n",
      "K -> L\n",
      "ABC -> D\n",
      "E -> F\n",
      "STU -> V\n",
      "TU -> V\n",
      "G -> H\n",
      "AB -> C\n",
      "J -> K\n",
      "FGH -> I\n",
      "MNOP -> Q\n",
      "VW -> X\n",
      "CD -> E\n",
      "TUVWX -> Y\n",
      "F -> G\n",
      "VWX -> Y\n",
      "LMNO -> P\n",
      "GHIJ -> K\n",
      "TUVWX -> Y\n",
      "JKL -> M\n",
      "LM -> N\n",
      "EFGHI -> J\n",
      "MNO -> P\n",
      "H -> I\n",
      "M -> N\n",
      "S -> T\n",
      "STU -> V\n",
      "QRST -> U\n",
      "PQR -> S\n",
      "RSTUV -> W\n",
      "ST -> U\n",
      "RSTUV -> W\n",
      "JKLM -> N\n",
      "T -> U\n",
      "CDE -> F\n",
      "HIJ -> K\n",
      "NOPQ -> R\n",
      "OPQ -> R\n",
      "EF -> G\n",
      "AB -> C\n",
      "CD -> E\n",
      "RST -> U\n",
      "STU -> V\n",
      "L -> M\n",
      "WXY -> Z\n",
      "STUVW -> X\n",
      "QRST -> U\n",
      "W -> X\n",
      "S -> T\n",
      "M -> N\n",
      "GH -> I\n",
      "QRST -> U\n",
      "FGH -> I\n",
      "PQRS -> T\n",
      "GH -> I\n",
      "DE -> F\n",
      "DE -> F\n",
      "GHIJK -> L\n",
      "Q -> R\n",
      "WX -> Y\n",
      "WX -> Y\n",
      "KLM -> N\n",
      "DE -> F\n",
      "EF -> G\n",
      "UVW -> X\n",
      "IJK -> L\n",
      "NO -> P\n",
      "QR -> S\n",
      "TU -> V\n",
      "RST -> U\n",
      "VW -> X\n",
      "A -> B\n",
      "DE -> F\n",
      "WXY -> Z\n",
      "CD -> E\n",
      "IJK -> L\n",
      "STUV -> W\n",
      "LMNOP -> Q\n",
      "X -> Y\n",
      "FGH -> I\n",
      "F -> G\n",
      "IJK -> L\n",
      "EFG -> H\n",
      "DEFG -> H\n",
      "NOP -> Q\n",
      "FG -> H\n",
      "RSTU -> V\n",
      "E -> F\n",
      "WXY -> Z\n",
      "GH -> I\n",
      "CD -> E\n",
      "IJ -> K\n",
      "TUVWX -> Y\n",
      "EFGH -> I\n",
      "DEFGH -> I\n",
      "BCDE -> F\n",
      "STUV -> W\n",
      "HI -> J\n",
      "GH -> I\n",
      "STUVW -> X\n",
      "ABC -> D\n",
      "S -> T\n",
      "LMNOP -> Q\n",
      "UVWX -> Y\n",
      "PQ -> R\n",
      "CDEF -> G\n",
      "E -> F\n",
      "TU -> V\n",
      "TUVWX -> Y\n",
      "GHIJ -> K\n",
      "JK -> L\n",
      "IJK -> L\n",
      "G -> H\n",
      "EFG -> H\n",
      "TU -> V\n",
      "FGHI -> J\n",
      "W -> X\n",
      "T -> U\n",
      "CDE -> F\n",
      "XY -> Z\n",
      "XY -> Z\n",
      "CDE -> F\n",
      "N -> O\n",
      "QRST -> U\n",
      "FGHIJ -> K\n",
      "PQ -> R\n",
      "I -> J\n",
      "GH -> I\n",
      "F -> G\n",
      "VWX -> Y\n",
      "ABC -> D\n",
      "GH -> I\n",
      "KLMN -> O\n",
      "X -> Y\n",
      "Q -> R\n",
      "NOPQR -> S\n",
      "HIJ -> K\n",
      "IJ -> K\n",
      "C -> D\n",
      "FG -> H\n",
      "JKLMN -> O\n",
      "TU -> V\n",
      "NOPQR -> S\n",
      "O -> P\n",
      "TU -> V\n",
      "MNOPQ -> R\n",
      "PQ -> R\n",
      "S -> T\n",
      "VWXY -> Z\n",
      "VWXY -> Z\n",
      "CD -> E\n",
      "BCDEF -> G\n",
      "OPQ -> R\n",
      "LMNO -> P\n",
      "HIJKL -> M\n",
      "STU -> V\n",
      "GHI -> J\n",
      "UVWX -> Y\n",
      "NOPQ -> R\n",
      "HIJK -> L\n",
      "NOP -> Q\n",
      "Q -> R\n",
      "HIJ -> K\n",
      "W -> X\n",
      "QR -> S\n",
      "UVWX -> Y\n",
      "H -> I\n",
      "ABC -> D\n",
      "RSTUV -> W\n",
      "VW -> X\n",
      "OP -> Q\n",
      "RSTUV -> W\n",
      "ABC -> D\n",
      "ABC -> D\n",
      "GHIJ -> K\n",
      "WXY -> Z\n",
      "BCDE -> F\n",
      "N -> O\n",
      "JK -> L\n",
      "X -> Y\n",
      "TUV -> W\n",
      "L -> M\n",
      "F -> G\n",
      "MN -> O\n",
      "JKLMN -> O\n",
      "G -> H\n",
      "BCDEF -> G\n",
      "LMN -> O\n",
      "N -> O\n",
      "V -> W\n",
      "BCDEF -> G\n",
      "KLM -> N\n",
      "ST -> U\n",
      "TUV -> W\n",
      "MN -> O\n",
      "JKLM -> N\n",
      "LM -> N\n",
      "U -> V\n",
      "FGH -> I\n",
      "TUV -> W\n",
      "C -> D\n",
      "HIJK -> L\n",
      "UVWX -> Y\n",
      "W -> X\n",
      "QR -> S\n",
      "PQR -> S\n",
      "STUVW -> X\n",
      "RSTU -> V\n",
      "TU -> V\n",
      "RSTU -> V\n",
      "JKL -> M\n",
      "JKL -> M\n",
      "RSTUV -> W\n",
      "GHI -> J\n",
      "V -> W\n",
      "CD -> E\n",
      "QRSTU -> V\n",
      "M -> N\n",
      "BCDE -> F\n",
      "WX -> Y\n",
      "K -> L\n",
      "VW -> X\n",
      "GHI -> J\n",
      "CD -> E\n",
      "XY -> Z\n",
      "HI -> J\n",
      "C -> D\n",
      "IJK -> L\n",
      "DEFG -> H\n",
      "UV -> W\n",
      "LM -> N\n",
      "X -> Y\n",
      "UV -> W\n",
      "I -> J\n",
      "NO -> P\n",
      "ABCD -> E\n",
      "K -> L\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "EFGHI -> J\n",
      "JK -> L\n",
      "TU -> V\n",
      "IJ -> K\n",
      "MNOPQ -> R\n",
      "C -> D\n",
      "IJKLM -> N\n",
      "VW -> X\n",
      "CDE -> F\n",
      "E -> F\n",
      "NOP -> Q\n",
      "OPQRS -> T\n",
      "FGHI -> J\n",
      "STUV -> W\n",
      "IJKLM -> N\n",
      "STUV -> W\n",
      "TUVWX -> Y\n",
      "RSTU -> V\n",
      "Model Accuracy: 98.40%\n",
      "['T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['V', 'W', 'X', 'Y'] -> Z\n",
      "['A', 'B', 'C', 'D'] -> E\n",
      "['C'] -> D\n",
      "['K', 'L', 'M', 'N'] -> O\n",
      "['B'] -> C\n",
      "['C', 'D', 'E', 'F', 'G'] -> H\n",
      "['Q', 'R'] -> S\n",
      "['T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['D', 'E', 'F', 'G', 'H'] -> I\n",
      "['B', 'C', 'D', 'E', 'F'] -> G\n",
      "['C', 'D', 'E', 'F'] -> G\n",
      "['C'] -> D\n",
      "['K', 'L', 'M'] -> N\n",
      "['B', 'C', 'D', 'E'] -> F\n",
      "['N', 'O'] -> P\n",
      "['P'] -> Q\n",
      "['W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['C'] -> D\n"
     ]
    }
   ],
   "source": [
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print sequence_in, '->', sequence_out\n",
    "    \n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=500, batch_size=batch_size, verbose=0)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "    x = numpy.reshape(x, (1, max_len, 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that although the model did not learn the alphabet perfectly from the randomly generated subsequences, it did very well. The model was not tuned and may require more training or a larger network, or both.\n",
    "\n",
    "This is a good natural extension to the “all sequential input examples in each batch” alphabet model learned above in that it can handle ad hoc queries, but this time of arbitrary sequence length (up to the max length)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
